{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Exercises\n",
    "\n",
    "<p>The end result of this exercise should be a file named <code>prepare.py</code> that defines\n",
    "the requested functions.</p>\n",
    "<p>In this exercise we will be defining some functions to prepare textual data.\n",
    "These functions should apply equally well to both the codeup blog articles and\n",
    "the news articles that were previously acquired.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from time import strftime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import acquire_codeup_blog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. <p>Define a function named <code>basic_clean</code>. It should take in a string and apply\n",
    "   some basic text cleaning to it:</p>\n",
    "<ul>\n",
    "<li>Lowercase everything</li>\n",
    "<li>Normalize unicode characters</li>\n",
    "<li>Replace anything that is not a letter, number, whitespace or a single quote.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(corpus):\n",
    "    '''\n",
    "    Basic text cleaning function  that  takes a corpus of text; lowercases everything; normalizes unicode characters; and replaces anything that is not a letter, number, whitespace or a single quote.\n",
    "    '''\n",
    "    lower_corpus = corpus.lower()\n",
    "    normal_corpus = unicodedata.normalize('NFKD', lower_corpus)\\\n",
    "    .encode('ascii', 'ignore')\\\n",
    "    .decode('utf-8', 'ignore')\n",
    "    basic_clean_corpus = re.sub(r\"[^a-z0-9'\\s]\", '', normal_corpus)\n",
    "    return(basic_clean_corpus)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Define a function named <code>tokenize</code>. It should take in a string and tokenize all the words in the string.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(string):\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "    return(tokenizer.tokenize(string, return_str=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Define a function named <code>stem</code>. It should accept some text and return the text after applying stemming to all the words.\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(text):\n",
    "    '''\n",
    "    Uses NLTK Porter stemmer object to return stems of words\n",
    "    '''\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    stems = [ps.stem(word) for word in text.split()]\n",
    "    stemmed_text = ' '.join(stems)\n",
    "    return stemmed_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Define a function named <code>lemmatize</code>. It should accept some text and return the text after applying lemmatization to each word.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    lemmas = [wnl.lemmatize(word) for word in text.split()]\n",
    "    lemmatized_text = ' '.join(lemmas)\n",
    "    return(lemmatized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Define a function named <code>remove_stopwords</code>. It should accept some text and return the text after removing all the stopwords.</p>\n",
    "<p>This function should define two optional parameters, <code>extra_words</code> and <code>exclude_words</code>. These parameters should define any additional stop words to\n",
    "include, and any words that we <em>don't</em> want to remove.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(string, extra_words = [], exclude_words = []):\n",
    "    '''\n",
    "    This function takes in a string, optional extra_words and exclude_words parameters\n",
    "    with default empty lists and returns a string.\n",
    "    '''\n",
    "    # Create stopword_list.\n",
    "    stopword_list = stopwords.words('english')\n",
    "    \n",
    "    # Remove 'exclude_words' from stopword_list to keep these in my text.\n",
    "    stopword_list = set(stopword_list) - set(exclude_words)\n",
    "    \n",
    "    # Add in 'extra_words' to stopword_list.\n",
    "    stopword_list = stopword_list.union(set(extra_words))\n",
    "\n",
    "    # Split words in string.\n",
    "    words = string.split()\n",
    "    \n",
    "    # Create a list of words from my string with stopwords removed and assign to variable.\n",
    "    filtered_words = [word for word in words if word not in stopword_list]\n",
    "    \n",
    "    # Join words in the list back into strings and assign to a variable.\n",
    "    string_without_stopwords = ' '.join(filtered_words)\n",
    "    \n",
    "    return string_without_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Use your data from the acquire to produce a dataframe of the news articles. Name the dataframe <code>news_df</code>.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Make another dataframe for the Codeup blog posts. Name the dataframe <code>codeup_df\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaredgodar/codeup-data-science/natural-language-processing-exercises/acquire_codeup_blog.py:14: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 14 of the file /Users/jaredgodar/codeup-data-science/natural-language-processing-exercises/acquire_codeup_blog.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  soup = BeautifulSoup(response.text)\n",
      "/Users/jaredgodar/codeup-data-science/natural-language-processing-exercises/acquire_codeup_blog.py:26: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 26 of the file /Users/jaredgodar/codeup-data-science/natural-language-processing-exercises/acquire_codeup_blog.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  soup = BeautifulSoup(response.text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to CSV file\n"
     ]
    }
   ],
   "source": [
    "codeup_df = acquire_codeup_blog.get_blog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Codeup Dallas Open House</td>\n",
       "      <td>Nov 30, 2021</td>\n",
       "      <td>Come join us for the re-opening of our Dallas ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Codeup’s Placement Team Continues Setting Records</td>\n",
       "      <td>Nov 19, 2021</td>\n",
       "      <td>Our Placement Team is simply defined as a grou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IT Certifications 101: Why They Matter, and Wh...</td>\n",
       "      <td>Nov 18, 2021</td>\n",
       "      <td>AWS, Google, Azure, Red Hat, CompTIA…these are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A rise in cyber attacks means opportunities fo...</td>\n",
       "      <td>Nov 17, 2021</td>\n",
       "      <td>In the last few months, the US has experienced...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Use your GI Bill® benefits to Land a Job in Tech</td>\n",
       "      <td>Nov 4, 2021</td>\n",
       "      <td>As the end of military service gets closer, ma...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title          date  \\\n",
       "0                           Codeup Dallas Open House  Nov 30, 2021   \n",
       "1  Codeup’s Placement Team Continues Setting Records  Nov 19, 2021   \n",
       "2  IT Certifications 101: Why They Matter, and Wh...  Nov 18, 2021   \n",
       "3  A rise in cyber attacks means opportunities fo...  Nov 17, 2021   \n",
       "4   Use your GI Bill® benefits to Land a Job in Tech   Nov 4, 2021   \n",
       "\n",
       "                                                post  \n",
       "0  Come join us for the re-opening of our Dallas ...  \n",
       "1  Our Placement Team is simply defined as a grou...  \n",
       "2  AWS, Google, Azure, Red Hat, CompTIA…these are...  \n",
       "3  In the last few months, the US has experienced...  \n",
       "4  As the end of military service gets closer, ma...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codeup_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. For each dataframe, produce the following columns:</p>\n",
    "<ul>\n",
    "<li><code>title</code> to hold the title</li>\n",
    "<li><code>original</code> to hold the original article/post content</li>\n",
    "<li><code>clean</code> to hold the normalized and tokenized original with the stopwords removed.</li>\n",
    "<li><code>stemmed</code> to hold the stemmed version of the cleaned data.</li>\n",
    "<li><code>lemmatized</code> to hold the lemmatized version of the cleaned data.</li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "9. Ask yourself: </p>\n",
    "<ul>\n",
    "<li>If your corpus is 493KB, would you prefer to use stemmed or lemmatized text?</li>\n",
    "<li>If your corpus is 25MB, would you prefer to use stemmed or lemmatized text?</li>\n",
    "<li>If your corpus is 200TB of text and you're charged by the megabyte for your hosted computational resources, would you prefer to use stemmed or lemmatized text?</li>\n",
    "</ul>\n",
    "</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "38cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
